{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 179 entries, 0 to 178\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Title             179 non-null    object\n",
      " 1   Organizations     179 non-null    object\n",
      " 2   Publication Year  179 non-null    int64 \n",
      " 3   Abstracts         179 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 5.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                               Title  \\\n",
       " 0  Cross-sectoral analysis of water usage in Thai...   \n",
       " 1  Relationships between causal factors affecting...   \n",
       " 2  A Study on the Evaluation Tendency of Thermal ...   \n",
       " 3  A Study on the Effect of Environment Sound on ...   \n",
       " 4  Water balance of pine forests: Synthesis of ne...   \n",
       " \n",
       "                                        Organizations  Publication Year  \\\n",
       " 0     Chulalongkorn University, Thammasat University              2022   \n",
       " 1                           Chulalongkorn University              2020   \n",
       " 2  Chulalongkorn University, Sugiyama Jogakuen Un...              2022   \n",
       " 3  Kyushu Sangyo University, Sugiyama Jogakuen Un...              2022   \n",
       " 4  Chulalongkorn University, Duke University, Swe...              2023   \n",
       " \n",
       "                                            Abstracts  \n",
       " 0  © 2018, Chulalongkorn University. All rights r...  \n",
       " 1  © 2019 by the authors.This research aims to an...  \n",
       " 2  © 2018 Web Portal IOP. All rights reserved.Cli...  \n",
       " 3  © 2018 Web Portal IOP. All rights reserved.In ...  \n",
       " 4  © 2018 Elsevier B.V.The forest hydrologic cycl...  ,\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file to analyze its structure\n",
    "file_path = 'resources/extracted_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the data to understand its structure\n",
    "data.head(), data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# import string\n",
    "\n",
    "# # Download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Preprocessing function\n",
    "# def preprocess_text(text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     # Lowercase text\n",
    "#     text = text.lower()\n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     # Tokenize words\n",
    "#     words = word_tokenize(text)\n",
    "#     # Remove stop words\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# # Apply preprocessing to the Abstracts column\n",
    "# data['Processed_Abstracts'] = data['Abstracts'].apply(preprocess_text)\n",
    "\n",
    "# # Display processed data\n",
    "# data[['Abstracts', 'Processed_Abstracts']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'sciencedirect_carbon_emissions.csv'. Fetched 179 results.\n"
     ]
    }
   ],
   "source": [
    "# Define a basic list of English stop words\n",
    "import string\n",
    "\n",
    "\n",
    "basic_stopwords = set([\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \n",
    "    \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"can't\", \n",
    "    \"come\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \n",
    "    \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \n",
    "    \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \n",
    "    \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "    \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \n",
    "    \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \n",
    "    \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \n",
    "    \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "    \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \n",
    "    \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \n",
    "    \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \n",
    "    \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "])\n",
    "\n",
    "# Updated preprocessing function\n",
    "def preprocess_text_basic(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize words\n",
    "    words = text.split()\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in basic_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the Abstracts column\n",
    "data['Processed_Abstracts'] = data['Abstracts'].apply(preprocess_text_basic)\n",
    "\n",
    "# Display processed data\n",
    "# data[['Abstracts', 'Processed_Abstracts']].head()\n",
    "\n",
    "\n",
    "# for i in range(0, 180):\n",
    "#     data['Organizations'][i] = 'Distirct {i}'.format(i=i)\n",
    "\n",
    "# data.head()\n",
    "df = pd.DataFrame(data[['Organizations' , 'Processed_Abstracts']])\n",
    "# Save to CSV\n",
    "if not df.empty:\n",
    "    df.to_csv('resources/sciencedirect_carbon_emissions.csv', index=False, encoding='utf-8')\n",
    "    print(f\"Data saved to 'sciencedirect_carbon_emissions.csv'. Fetched {len(df)} results.\")\n",
    "else:\n",
    "    print(\"No data fetched. Check the scraping process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['energy', 'carbon', 'development'],\n",
       " ['climate', 'change', 'research'],\n",
       " ['model', 'study', 'waste'],\n",
       " ['co2', 'carbon', 'energy'],\n",
       " ['climate', 'energy', 'change']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the processed abstracts\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(data['Processed_Abstracts'])\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Assuming 5 topics for simplicity\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Extract topic words\n",
    "def get_topics(model, feature_names, n_top_words=3):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(top_features)\n",
    "    return topics\n",
    "\n",
    "# Get the feature names and topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = get_topics(lda, feature_names)\n",
    "\n",
    "# Display topics\n",
    "topics\n",
    "\n",
    "# df = pd.DataFrame(topics)\n",
    "\n",
    "# # Save to CSV\n",
    "# if not df.empty:\n",
    "#     df.to_csv('topics.csv', index=False, encoding='utf-8')\n",
    "#     print(f\"Data saved to 'topics.csv'. Fetched {len(df)} results.\")\n",
    "# else:\n",
    "#     print(\"No data fetched. Check the scraping process.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
